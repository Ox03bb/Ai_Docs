{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Neural Network From Secratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"display:flex;justify-content:center\">\n",
    "        <img title=\"a title\" alt=\"Alt text\" src=\"./img/img_02.png\" style=\"width:50vw;filter: invert(0.85);\">\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this project i will create a multy layer perceptron network from scratch using only numpy libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "        def liner(self, x):\n",
    "            return x\n",
    "        def step(self, x):\n",
    "            return 1 if x > 0 else 0\n",
    "        def sigmoid(self, x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        def relu(self, x):\n",
    "            return max(0, x) \n",
    "        def softmax(self, x):\n",
    "            return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost: \n",
    "        @staticmethod  \n",
    "        def loss_function(w,b,x,y):\n",
    "            return y - (w*x + b)\n",
    "        @staticmethod    \n",
    "        def mse(self, w,b,x,y):\n",
    "            \"Mean Squared Error (MSE)\"\n",
    "            return np.mean(np.square(y - (w*x + b)))\n",
    "            # return np.mean(np.square(self.loss_function(w,b,x,y)))\n",
    "        @staticmethod \n",
    "        def mae(self, w,b,x,y):\n",
    "            \"Mean Absolute Error (MAE)\"\n",
    "            return np.mean(np.abs(self.loss_function(w,b,x,y)))\n",
    "        \n",
    "        @staticmethod \n",
    "        def log(self, w,b,x,y):\n",
    "            \"Binary Cross-Entropy (Log Loss)\"\n",
    "            return -np.mean(y * np.log(self.loss_function(w,b,x,y)) + (1 - y) * np.log(1 - self.loss_function(w,b,x,y)))\n",
    "        @staticmethod \n",
    "        def cce(self, w,b,x,y):\n",
    "            \"Categorical Cross-Entropy\"\n",
    "            return -np.sum(y * np.log(self.loss_function(w,b,x,y)))\n",
    "        \n",
    "        #! cost derivative for dw and db\n",
    "        \n",
    "        @staticmethod \n",
    "        def d_mse(w,b,x,y):\n",
    "            db = 0\n",
    "            dm = 0\n",
    "            n =0\n",
    "            while x[n]:\n",
    "                n = n + 1\n",
    "           \n",
    "            for i in range(n): \n",
    "                dm += -2*x[i] * (y[i] - (w*x[i] + b)) \n",
    "                db += -2*(y[i] - (w*x[i] + b))\n",
    "            return dm/n, db/n           \n",
    "        \n",
    "        \n",
    "        class D_Cost:\n",
    "            @staticmethod \n",
    "            def d_mse(w,b,x,y):\n",
    "                db = 0\n",
    "                dm = 0\n",
    "                n =len(x)\n",
    "               \n",
    "\n",
    "                for i in range(n): \n",
    "                    dm += -2*x[i] * (y[i] - (w*x[i] + b)) \n",
    "                    db += -2*(y[i] - (w*x[i] + b))\n",
    "                return dm/n, db/n           \n",
    "            \n",
    "            @staticmethod \n",
    "            def d_mae(w,b,x,y):\n",
    "                db = 0\n",
    "                dw = 0\n",
    "                n = len(x)\n",
    "\n",
    "                for i in range(n): \n",
    "                    dw += -np.sign(y[i] - (w*x[i] + b)) * x[i]\n",
    "                    db += -np.sign(y[i] - (w*x[i] + b))\n",
    "                return dw/n, db/n\n",
    "            @staticmethod \n",
    "            def d_log(w,b,x,y):\n",
    "                \n",
    "                db = 0\n",
    "                dw = 0\n",
    "                n = len(x)\n",
    "\n",
    "                for i in range(n): \n",
    "                    dw += -x[i] * (y[i] - (w*x[i] + b)) \n",
    "                    db += -(y[i] - (w*x[i] + b))\n",
    "                return dw/n, db/n\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- Perceptron (Bugs)\n",
    "\n",
    "in this part i will create the basic unit of the `neural network` which is the `perceptron`, the perceptron is a simple unit that takes a vector of inputs and multiply them by a vector of weights and then apply an `activation function` to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptorn:\n",
    "    \n",
    "    def __init__(self, input_num=1, activation='liner', cost='mse'):\n",
    "        self.activation = self.activation[activation]\n",
    "        self.weights = np.zeros(input_num)\n",
    "        self.bias = 0\n",
    "      \n",
    "        \n",
    "    \n",
    "    activation = {'liner': Activation.liner, 'step': Activation.step, 'sigmoid': Activation.sigmoid, 'relu': Activation.relu}\n",
    "    cost = {'mse': Cost.mse, 'mae': Cost.mae, 'log': Cost.log, 'cce': Cost.cce}\n",
    " \n",
    "    def get_weights(self):\n",
    "        return self.weights ,self.bias\n",
    "    \n",
    "    def set_weights(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.activation(np.dot(self.weights, x) + self.bias)\n",
    "    \n",
    "    def gradient_Descent(x, y, learning_rate=0.1, epochs=100):\n",
    "        w= 0\n",
    "        b=2\n",
    "        \n",
    "        pd_logs = []\n",
    "        wb_logs = [[w,b]]\n",
    "        cost_logs = []    \n",
    "        # n = len(x)\n",
    "\n",
    "        print(x)\n",
    "        for i in range(epochs):\n",
    "            \n",
    "            dw,db = Cost.d_mse(w=w,b=b,x=x,y=y)\n",
    "            pd_logs.append([dw, db])\n",
    "            \n",
    "            w = w - learning_rate * dw\n",
    "            b = b - learning_rate * db\n",
    "            wb_logs.append([w, b])\n",
    "            \n",
    "            cost = Cost.mse(w,b,x,y)\n",
    "            cost_logs.append(cost)\n",
    "            \n",
    "            if dw == 0 and db == 0:\n",
    "                break\n",
    "            \n",
    "        return w, b , pd_logs, wb_logs ,cost_logs\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, x, y, epochs=100, lr=0.01 , cost='mse'):\n",
    "        # use the cost function to calculate the error\n",
    "        \n",
    "        w_logs = []\n",
    "        b_logs = []\n",
    "        \n",
    "        self.bias = 1\n",
    "        self.weights = 1\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            for i in range(len(x)):\n",
    "                self.weights -= lr * self.cost[cost](self,w = self.weights, b =self.bias, x=x, y=y)\n",
    "                w_logs.append(self.weights)\n",
    "                self.bias -= lr * self.cost[cost](self,w=self.weights,b= self.bias,x= x,y= y)\n",
    "                b_logs.append(self.bias)\n",
    "        return f'weights: {self.weights}, bias: {self.bias}'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'weights: {self.weights}, bias: {self.bias}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [0.], bias: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Perceptorn' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m p \u001b[38;5;241m=\u001b[39m Perceptorn()\n\u001b[0;32m      2\u001b[0m p\u001b[38;5;241m.\u001b[39mget_weights()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_Descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# p.train(x=[1,2,3,4,5], y=[1,2,3,4,5], epochs=80, lr=0.01, cost='mse')\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[62], line 35\u001b[0m, in \u001b[0;36mPerceptorn.gradient_Descent\u001b[1;34m(x, y, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 35\u001b[0m     dw,db \u001b[38;5;241m=\u001b[39m \u001b[43mCost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_mse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     pd_logs\u001b[38;5;241m.\u001b[39mappend([dw, db])\n\u001b[0;32m     38\u001b[0m     w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m-\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m dw\n",
      "Cell \u001b[1;32mIn[61], line 31\u001b[0m, in \u001b[0;36mCost.d_mse\u001b[1;34m(w, b, x, y)\u001b[0m\n\u001b[0;32m     29\u001b[0m dm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     30\u001b[0m n \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[0;32m     32\u001b[0m     n \u001b[38;5;241m=\u001b[39m n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n): \n",
      "\u001b[1;31mTypeError\u001b[0m: 'Perceptorn' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "p = Perceptorn()\n",
    "p.get_weights()[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "p.gradient_Descent([1,2,3,4,5], [1,2,3,4,5])\n",
    "\n",
    "# p.train(x=[1,2,3,4,5], y=[1,2,3,4,5], epochs=80, lr=0.01, cost='mse')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"display:flex;justify-content:center\">\n",
    "        <img title=\"a title\" alt=\"Alt text\" src=\"./img/nn.png\" style=\"width:50vw;filter: invert(0.92);\">\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Z^1 = \\left( \\begin{bmatrix} b_1^0 & b_2^0 & b_3^0 & b_4^0 \\end{bmatrix} + \\begin{bmatrix} a_1^0 & a_2^0 \\end{bmatrix} \\right) \\begin{bmatrix} \\theta_{11}^0 & \\theta_{21}^0 & \\theta_{31}^0 & \\theta_{41}^0 \\\\ \\theta_{12}^0 & \\theta_{22}^0 & \\theta_{32}^0 & \\theta_{42}^0 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "Z^2 = \\left( \\begin{bmatrix} b_1^1 & b_2^1 & b_3^1 \\end{bmatrix} + \\begin{bmatrix} a_1^1 & a_2^1 & a_3^1 & a_4^1 \\end{bmatrix} \\right) \\begin{bmatrix} \\theta_{11}^1 & \\theta_{21}^1 & \\theta_{31}^1 \\\\ \\theta_{12}^1 & \\theta_{22}^1 & \\theta_{32}^1 \\\\ \\theta_{13}^1 & \\theta_{23}^1 & \\theta_{33}^1 \\\\ \\theta_{14}^1 & \\theta_{24}^1 & \\theta_{34}^1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^3 = \\left( \\begin{bmatrix} b_1^2 \\end{bmatrix} + \\begin{bmatrix} a_1^2 & a_2^2 & a_3^2 \\end{bmatrix} \\right) \\begin{bmatrix} \\theta_{11}^2 \\\\ \\theta_{12}^2 \\\\ \\theta_{13}^2 \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "   def __init__(self, input_num=1,layers_num = 3 , node_num = [4,3,1] , activation='liner', cost='mse'):\n",
    "        # self.NN this var is a 3d araay contains the layer number ,node number , and wights\n",
    "        \n",
    "        self.NN =  [[[]]]\n",
    "        \n",
    "        for l in range(layers_num):\n",
    "            for n in range(node_num):\n",
    "                for w in range(node_num[n+1]):\n",
    "                    self.NN[l][n][w] =     \n",
    "                \n",
    "        self.input_layer = np.zeros(input_num)\n",
    "        self.activation = self.activation[activation]\n",
    "        self.weights = np.zeros(input_num)\n",
    "        self.bias = 0\n",
    "        \n",
    "        self.thetas_layer0= np.random.rand(self.inputs_in_layer0.shape[1] + 1,self.nodes_in_layer1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
